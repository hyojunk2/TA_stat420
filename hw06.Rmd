---
title: 'STAT 420: Homework 6'
author: "Wenke Huang, Summer 2017"
date: 'Due: Monday, July 24 by 11:50 PM CDT'
output:
  html_document:
    theme: readable
    toc: yes
---

# Directions

Students are encouraged to work together on homework using the discussion boards. However, sharing, copying or providing any part of a homework solution or code is an infraction of the University's rules on Academic Integrity. Any violation will be punished as severely as possible.

- Your assignment must be submitted through the [submission link](https://compass2g.illinois.edu/webapps/assignment/uploadAssignment?content_id=_2663002_1&course_id=_31866_1&group_id=&mode=cpview) on **Compass 2g.** You are required to attach two (and only two) files to the *same* submission:
    - Your RMarkdown file which should be saved as `hw06_yourNetID.Rmd`. For example `hw06_dunger.Rmd`.
    - The result of knitting your RMarkdown file as `hw06_yourNetID.html`. For example `hw06_dunger.html`.
    - Any outside data provided as a `.csv` file used for the homework.
- To submit the two files, you must "zip" them together into a single `zip` file, and then submit that one file.
- Your resulting `.html` file will be considered a "report" which is the material that will determine the majority of your grade. Be sure to visibly include all `R` code and output that is relevant to answering the exercises. (You do not need to include irrelevant code you tried that resulted in error or did not answer the question correctly.)
- You are granted an unlimited number of submissions, but only the last submission *before* the deadline will be viewed and graded.
- If you use [this `.Rmd` file as a template](hw06.Rmd), be sure to remove the directions section, and consider removing `eval = FALSE` from any code chunks provided. (If you would like to run that code as part of your assignment.)
- Your `.Rmd` file should be written such that, if it is placed in a folder with any data you are asked to import, it will Knit properly without modification. 
- Unless otherwise stated, you may use `R` for each of the exercises.
- Be sure to read each exercise carefully!
- Include your Name and NetID in the final document, not only in your filenames.

# Assignment

## Exercise 1 (Writing Functions)

**(a)** Write a function that takes as input a model object (variable) fit via `lm()` and outputs a fitted versus residuals plot. Also create arguments `pointcol` and `linecol` which control the point and line colors respectively. Code the plot to add a horizontal line at $y = 0$ and label the $x$-axis "Fitted" and the $y$-axis "Residuals".

```{r}
library(ggplot2)
fitted_vs_residuals <- function(model, pointcol="black", linecol="black"){
  ggplot() +
    geom_point(mapping = aes(x = model$fitted.values,
                             y = model$residuals),
               color = pointcol) +
    xlab("Fitted Values") + ylab("Residuals") +
    geom_hline(yintercept = 0, color = linecol)
}
```

**(b)** Write a function that takes as input a model fit via `lm()` plots a Normal Q-Q plot of the residuals. Also create arguments `pointcol` and `linecol` which control the point and line colors respectively. Code the plot to add the line from `qqline()`.

```{r}
Normal_QQ <- function(model, pointcol = "black", linecol = "black"){
  # resid = rstandard(model)
  resid = model$residuals
  point_1 = c(qnorm(0.25), quantile(resid, probs = 0.25))
  point_2 = c(qnorm(0.75), quantile(resid, probs = 0.75))
  
  slp = (point_2[2]-point_1[2])/(point_2[1]-point_1[1])
  int = point_1[2]-slp*point_1[1]
  ggplot() +
    geom_qq(mapping = aes(sample = resid),
            color = pointcol) +
    geom_abline(slope = slp, intercept = int, color = linecol)
}
```

**(c)** Test your two functions above on the `test_fit` model. For both functions, specify point and line colors that are not black.

```{r}
set.seed(420)
test_data = data.frame(x = runif(20, 0, 10),
                       y = rep(0, 20))
test_data$y = 5 + 2 * test_data$x + rnorm(20)
test_fit = lm(y ~ x, data = test_data)
fitted_vs_residuals(test_fit, pointcol = "blue", linecol="red")
Normal_QQ(test_fit, pointcol = 'blue', linecol = "red")
```

## Exercise 2 (Swiss Fertility Data)

For this exercise we will use the `swiss` data which can be found in the `faraway` package. After loading the `faraway` package, use `?swiss` to learn about this dataset.

```{r, message = FALSE, warning = FALSE}
library(faraway)
```

**(a)** Fit an additive multiple regression model with `Fertility` as the response, and the remaining variables in the `swiss` dataset as predictors. Output the estimated regression coefficients for this model.

```{r}
model_2a <- lm(Fertility ~., data=swiss)
summary(model_2a)$coefficients
```

**(b)** Check the constant variance assumption for this model. Do you feel it has been violated? Justify your answer.

```{r}
fitted_vs_residuals(model_2a)
```

**(c)** Check the normality assumption for this model. Do you feel it has been violated? Justify your answer.

```{r}
Normal_QQ(model_2a)
```

**(d)** Check for any high leverage observations. Report any observations you determine to have high leverage.

```{r}
hatvalues(model_2a)
```

**(e)** Check for any influential observations. Report any observations you determine to be influential.

```{r}
Cooks_dis = cooks.distance(model_2a)
which(Cooks_dis > 4/nrow(swiss))
```

**(f)** Refit the additive multiple regression model without any points you identified as influential. Compare the coefficients of this fitted model to the previously fitted model.

```{r}
index = !(rownames(swiss) %in% c("Porrentruy", "Sierre", "Neuchatel", "Rive Droite", "Rive Gauche"))
swiss_subset = swiss[index, ]
model_2f <- lm(Fertility ~., data = swiss_subset)
summary(model_2f)$coefficients
```

**(g)** Create a data frame that stores the observations that were "removed" because they were influential. Use the two models you have fit to make predictions with these observations. Comment on the difference between these two sets of predictions.

```{r}
influ <- swiss[!index, ]
predict(model_2a, newdata = influ)
predict(model_2f, newdata = influ)
```

## Exercise 3 (TV is Healthy?)

For this exercise we will use the `tvdoctor` data which can be found in the `faraway` package. After loading the `faraway` package, use `?tvdoctor` to learn about this dataset.

```{r, message = FALSE, warning = FALSE}
library(faraway)
```

**(a)** Fit a simple linear regression with `life` as the response and `tv` as the predictor. Plot a scatterplot and add the fitting line. Check the assumptions of this model.

```{r}
model_3a <- lm(life ~ tv, data=tvdoctor)
ggplot() +
  geom_point(data=tvdoctor,
             mapping = aes(x =tv,
                           y =life)) +
  geom_abline(slope = model_3a$coefficients[2],
              intercept = model_3a$coefficients[1])
fitted_vs_residuals(model_3a)
Normal_QQ(model_3a)
```

**(b)** Fit higher order polynomial models of degree 3, 5, and 7. For each, plot a fitted versus residuals plot and comment on the constant variance assumption. Based on those plots, which of these three models to you think are acceptable? Use a statistical test(s) to compare the models you just chose. Based on the test, which is preferred? Check the normality assumption of this model. Identify any influential observations of this model.

```{r}
model_3b3 <- lm(life ~ poly(tv, 3, raw=TRUE), data=tvdoctor)
fitted_vs_residuals(model_3b3)

model_3b5 <- lm(life ~ poly(tv, 5, raw=TRUE), data=tvdoctor)
fitted_vs_residuals(model_3b5)

model_3b7 <- lm(life ~ poly(tv, 7, raw=TRUE), data=tvdoctor)
fitted_vs_residuals(model_3b7)

anova(model_3b3, model_3b5)
anova(model_3b3, model_3b7)
anova(model_3b5, model_3b7)

Normal_QQ(model_3b5)
```

## Exercise 4 (Brains)

The data set `mammals` from the `MASS` package contains the average body weight in kilograms $(x)$ and the average brain weight in grams $(y)$ for $62$ species of land mammals. Use `?mammals` to learn more.

```{r, message = FALSE, warning = FALSE}
library(MASS)
```

**(a)** What are the smallest and largest body weights in the dataset?

```{r}
min(mammals$body)
max(mammals$body)
mammals[which.min(mammals$body), ]
mammals[which.max(mammals$body), ]
```

**(b)** What are the smallest and largest brain weights in the dataset?

```{r}
min(mammals$brain)
max(mammals$brain)
mammals[which.min(mammals$brain), ]
mammals[which.max(mammals$brain), ]
```

**(c)** Plot average brain weight $(y)$ vs. the average body weight $(x)$.

```{r}
ggplot() +
  geom_point(data = mammals,
             mapping = aes(x = body,
                           y = brain))
```

**(d)** Fit a linear model with `brain` as the response and `body` as the predictor. Test for significance of regression. Do you think this is an appropriate model?

```{r}
model_4d <- lm(brain ~ body, data = mammals)
anova(model_4d)
fitted_vs_residuals(model_4d)
Normal_QQ(model_4d)
```

**(e)** Since the body weights do range over more than one order of magnitude and are strictly positive, we will use $\log(\text{body weight})$ as our *predictor*, with no further justification. Use the Box-Cox method to verify that $\log(\text{brain weight})$ is then a "recommended" transformation of the *response* variable. That is, verify that $\lambda = 0$ is among the "recommended" values of $\lambda$ when considering,

\[
g_\lambda(y) = \beta_0 + \beta_1 \log(\text{body weight})+\epsilon
\]

Please include the relevant plot in your results, using an appropriate zoom onto the relevant values.

```{r}
model_4e <- lm(brain ~ log(body), data = mammals)
boxcox(model_4e, lambda = seq(-.1, .1, by = .001), plotit = TRUE)
```

**(f)** Fit the model justified in part **(e)**. That is fit a model with $\log(\text{brain weight})$ as the response and $\log(\text{body weight})$ as a predictor. Plot $\log(\text{brain weight})$ versus $\log(\text{body weight})$ and add the regression line to the plot. Does a linear relationship seem to be appropriate here?

```{r}
model_4f <- lm(log(brain) ~ log(body), data = mammals)
ggplot() +
  geom_point(data = mammals,
             mapping = aes(x = log(body),
                           y = log(brain))) +
  geom_abline(intercept = model_4f$coefficients[1],
              slope = model_4f$coefficients[2])
```

**(g)** Use a Q-Q plot to check the normality of the errors for the model fit in part **(f)**.

```{r}
Normal_QQ(model_4f)
```

**(h)** Use the model from part **(f)** to predict the brain weight of a male Pikachu which has a body weight 13.4 pounds. (Pikachu would be mammals, right?) Construct a 99% prediction interval.

```{r}
exp(predict(model_4f, newdata = data.frame(body = 13.4/2.2),
            interval = "prediction",
            level = .99))
```

## Exercise 5 (EPA Emissions Data, revisited)

For this exercise we will again use the data stored in [`epa2015.csv`](epa2015.csv) which we saw in the [last homework](https://compass2g.illinois.edu/bbcswebdav/pid-2660242-dt-content-rid-28248825_1/xid-28248825_1). It contains detailed descriptions of 4,411 vehicles manufactured in 2015 used for fuel economy testing [as performed by the Environment Protection Agency]( https://www3.epa.gov/otaq/tcldata.htm).

**(a)** Recall the model we had finished with last time:

```{r}
epa2015 = read.csv("epa2015.csv")
co2_int = lm(CO2 ~ horse * type, data = epa2015)
```

Which looked like this:

```{r}
plot(CO2 ~ horse, data = epa2015, col = type)

int_coef = summary(co2_int)$coef[,1]

int_both    = int_coef[1]
int_car     = int_coef[1] + int_coef[3]
int_truck   = int_coef[1] + int_coef[4]

slope_both  = int_coef[2]
slope_car   = int_coef[2] + int_coef[5]
slope_truck = int_coef[2] + int_coef[6]

abline(int_both, slope_both, lwd = 3, col = "black")
abline(int_car, slope_car, lwd = 3, col = "red")
abline(int_truck, slope_truck, lwd = 3, col = "green")
```

Create a fitted vs residuals plot for this model. Do you believe the constant variance assumption has been violated?

```{r}
fitted_vs_residuals(co2_int)
```

**(b)** Fit the same model as **(a)** but with a logged response. Create a fitted vs residuals plot for this model. Compare to the previous. Do you believe the constant variance assumption has been violated? Any other assumptions?

```{r}
model_5b <- lm(log(CO2) ~ horse * type, data = epa2015)
fitted_vs_residuals(model_5b)
```

**(c)** Fit a model that has all of the terms from the model in **(b)** as well as a quadratic term for `horse`. Again use `log(CO2)` as the response. Create a fitted vs residuals plot for this model. Compare to the previous. Comment on model assumptions.

```{r}
model_5c <- lm(log(CO2) ~ horse * type + I(horse^2), data = epa2015)
fitted_vs_residuals(model_5c)
```

**(d)** Perform further analysis of the model fit in part **(c)**. Can you find any violations of assumptions?

```{r}
Normal_QQ(model_5c)
```

## Exercise 6 (Why Bother?)

**Why** do we care about violations of assumptions? One key reason is that the distributions of the parameters that we have used are all reliant on these assumptions. When the assumptions are violated, the distributional results are not correct, so are tests are garbage. **Garbage In, Garbage Out!**

Consider the following setup that we will use for the remainder of the exercise. We choose a sample size of 50.

```{r}
n = 50
set.seed(420)
x_1 = runif(n, 0, 10)
x_2 = runif(n, -5, 5)
```

Consider the model,

\[
Y = 2 + 1 x_1 + 0 x_2 + \epsilon.
\]

That is,

- $\beta_0$ = 2,
- $\beta_1$ = 1,
- $\beta_2$ = 0.

We now simulate `y_1` in a manner that doesn't violate any assumptions, which we verify. In this case $\epsilon \sim N(0, 1).$

```{r}
y_1   = 2 + x_1 + 0 * x_2 + rnorm(n)
fit_1 = lm(y_1 ~ x_1 + x_2)
qqnorm(resid(fit_1), col = "dodgerblue")
qqline(resid(fit_1), col = "darkorange", lwd = 2)
shapiro.test(resid(fit_1))
```

Then, we simulate `y_2` in a manner that **does** violate assumptions, which we verify. In this case $\epsilon \sim N(0, \sigma = |x_2|).$

```{r}
y_2   = 2 + x_1 + 0 * x_2 + rnorm(n, 0, abs(x_2))
fit_2 = lm(y_2 ~ x_1 + x_2)
qqnorm(resid(fit_2), col = "dodgerblue")
qqline(resid(fit_2), col = "darkorange", lwd = 2)
shapiro.test(resid(fit_2))
```

**(a)** Use the following code after changing `uin` to your UIN.

```{r}
num_sims = 1000
p_val_1 = rep(0, num_sims)
p_val_2 = rep(0, num_sims)
uin = 123456789
set.seed(uin)
```

Repeat the above process of generating `y_1` and `y_2` as defined above, and fit models with each as the response `1000` times. Each time, store the p-value for testing,

\[
\beta_2 = 0,
\]

using both model, in the appropriate variables defined above. (You do not need to use a data frame as we have in the past.)

```{r}
for(i in 1:num_sims) {
  y_1        = 2 + x_1 + 0 * x_2 + rnorm(n)
  fit_1      = lm(y_1 ~ x_1 + x_2)
  p_val_1[i] = summary(fit_1)$coef[3,4]
  
  y_2        = 2 + x_1 + 0 * x_2 + rnorm(n, 0, abs(x_2))
  fit_2      = lm(y_2 ~ x_1 + x_2)
  p_val_2[i] = summary(fit_2)$coef[3,4]
}
```

**(b)** What proportion of the `p_val_1` values are less than 0.05? Less than 0.10? What proportion of the `p_val_2` values are less than 0.05? Less than 0.10? Briefly explain these results.

```{r}
mean(p_val_1 < 0.05)
mean(p_val_1 < 0.1)
mean(p_val_2 < 0.05)
mean(p_val_2 < 0.1)
```

## Exercise 7 (Bigger Is Better?)

Consider the true model,

\[
Y = 3 - 4 x + \epsilon,
\]

where $\epsilon \sim N(\mu = 0, \sigma = 9)$.

We can simulate observations from this model. We choose a sample size of 40.

```{r}
n = 40
set.seed(420)
x = runif(n, 0 , 10)
y = 3 - 4 * x + rnorm(n, 0 , 3)
```

Consider two models, one small, one big. The small fits a SLR model. The big fits a polynomial model of degree 10.

```{r}
fit_slr = lm(y ~ x)
fit_big = lm(y ~ poly(x, 10))
```

The big model has a smaller RMSE.

```{r}
mean(resid(fit_slr) ^ 2)
mean(resid(fit_big) ^ 2)
```

However, it is not significant when compared to the small.

```{r}
anova(fit_slr, fit_big)
```

By plotting the the data and adding the two models, we see the the degree 10 polynomial is *very* wiggly. 

```{r}
plot(x, y, pch = 20, cex = 2)
abline(fit_slr, col = "darkorange", lwd = 3)
lines(seq(0, 10, 0.01), 
      predict(fit_big, newdata = data.frame(x = seq(0, 10, 0.01))), 
      col = 'dodgerblue', lwd = 3) 
```

**(a)** Use the following code after changing `uin` to your UIN.

```{r}
num_sims = 1000
rmse_slr = rep(0, num_sims)
rmse_big = rep(0, num_sims)
pval     = rep(0, num_sims)
```

Repeat the above process, keeping `x` the same, the re-generating `y` and fitting the SLR and big models `1000` times. Each time, store the RMSE of each model, p-value for comparing the two. (In the appropriate variables defined above.)

```{r}
uin = 123456789
set.seed(uin)
for(i in 1:num_sims){
  y = 3-4*x+rnorm(n, 0, 3)
  fit_slr = lm(y ~ x)
  fit_big = lm(y ~ poly(x, 10))
  rmse_slr[i] = mean(fit_slr$residuals^2)
  rmse_big[i] = mean(fit_big$residuals^2)
  pval[i] = anova(fit_slr, fit_big)$P[2]
}
```

**(b)** What proportion of the RMSEs of the SLR model are smaller than the big model?

```{r}
mean(rmse_slr < rmse_big)
```

**(c)** What proportion of the p-values are less than 0.05?

```{r}
mean(pval < 0.05)
```

**(d)** Do you think bigger is better?

