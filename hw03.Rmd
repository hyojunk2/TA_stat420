---
title: 'STAT 420: Homework 3'
author: "Wenke Huang, Summer 2017"
date: 'Due: Monday, July 3 by 11:50 PM CDT'
output:
  html_document:
    theme: readable
    toc: yes
---

# Directions

Students are encouraged to work together on homework using the discussion boards. However, sharing, copying or providing any part of a homework solution or code is an infraction of the University's rules on Academic Integrity. Any violation will be punished as severely as possible.

- Your assignment must be submitted through the [submission link](https://compass2g.illinois.edu/webapps/assignment/uploadAssignment?content_id=_2650489_1&course_id=_31866_1&group_id=&mode=cpview) on **Compass 2g.** You are required to attach two (and only two) files to the *same* submission:
    - Your RMarkdown file which should be saved as `hw03_yourNetID.Rmd`. For example `hw03_dunger.Rmd`.
    - The result of knitting your RMarkdown file as `hw03_yourNetID.html`. For example `hw03_dunger.html`.
    - Any outside data provided as a `.csv` file used for the homework.
- To submit the two files, you must "zip" them together into a single `zip` file, and then submit that one file.
- Your resulting `.html` file will be considered a "report" which is the material that will determine the majority of your grade. Be sure to visibly include all `R` code and output that is relevant to answering the exercises. (You do not need to include irrelevant code you tried that resulted in error or did not answer the question correctly.)
- You are granted an unlimited number of submissions, but only the last submission *before* the deadline will be viewed and graded.
- If you use [this `.Rmd` file as a template](hw03.Rmd), be sure to remove the directions section, and consider removing `eval = FALSE` from any code chunks provided. (If you would like to run that code as part of your assignment.)
- Your `.Rmd` file should be written such that, if it is placed in a folder with any data you are asked to import, it will Knit properly without modification. 
- Unless otherwise stated, you may use `R` for each of the exercises.
- Be sure to read each exercise carefully!
- Include your Name and NetID in the final document, not only in your filenames.

# Assignment

## Exercise 1 (Using `lm` for Inference)

For this exercise we will again use the `faithful` dataset. Remember, this is a default dataset in `R`, so there is no need to load it. You should use `?faithful` to refresh your memory about the background of this dataset about the duration and waiting times of eruptions of [the Old Faithful geyser](http://www.yellowstonepark.com/about-old-faithful/) in [Yellowstone National Park](https://en.wikipedia.org/wiki/Yellowstone_National_Park).

**(a)** Fit the following simple linear regression model in `R`. Use the eruption duration as the response and waiting time as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `faithful_model`. Use a $t$ test to test the significance of the regression. Report the following:

- The null and alternative hypotheses.
- The value of the test statistic.
- The p-value of the test.
- A statistical decision at $\alpha = 0.01$.
- A conclusion in the context of the problem.

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

```{r}
faithful_model <- lm(eruptions ~ waiting, data = faithful)
summary(faithful_model)
```

**(b)** Calculate a 99% confidence interval for $\beta_1$. Give an interpretation of the interval in the context of the problem.

```{r}
confint(faithful_model, level = .99)
```

**(c)** Calculate a 90% confidence interval for $\beta_0$. Give an interpretation of the interval in the context of the problem.

```{r}
confint(faithful_model, level = 0.90)
```

**(d)** Use a 95% confidence interval to estimate the mean eruption duration for waiting times of 75 and 80 minutes. Which of the two intervals is wider? Why?

```{r}
predict(faithful_model,
        newdata = data.frame(waiting = c(75, 80)),
        interval = "confidence",
        level = .95)
```

**(e)** Use a 95% prediction interval to predict the eruption duration for waiting times of 75 and 100 minutes.

```{r}
predict(faithful_model,
        newdata = data.frame(waiting = c(75, 100)),
        interval = "prediction",
        level = .95)
```

**(f)** Create a scatterplot of the data. Add the regression line, 95% confidence bands, and 95% prediction bands.

```{r, warning=FALSE}
library(ggplot2)
CI = data.frame(predict(faithful_model,
                        interval = "confidence",
                        level = .95))
PI = data.frame(predict(faithful_model,
                        interval = "prediction",
                        level = .95))
ggplot() +
  geom_point(data = faithful,
             mapping = aes(x = waiting,
                           y = eruptions)) +
  geom_abline(slope = faithful_model$coefficients[2],
              intercept = faithful_model$coefficients[1]) +
  geom_line(data = CI,
            mapping = aes(x = faithful$waiting,
                          y = lwr),
            linetype = "dashed") +
  geom_line(data = CI,
            mapping = aes(x = faithful$waiting,
                          y = upr),
            linetype = "dashed") +
  geom_line(data = PI,
            mapping = aes(x = faithful$waiting,
                          y = lwr),
            linetype = "dotted") +
  geom_line(data = PI,
            mapping = aes(x = faithful$waiting,
                          y = upr),
            linetype = "dotted")
```

## Exercise 2 (Using `lm` for Inference)

For this exercise we will again use the `diabetes` dataset which can be found in the `faraway` package.

**(a)** Fit the following simple linear regression model in `R`. Use the total cholesterol as the response and weight as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `cholesterol_model`. Use an $F$ test to test the significance of the regression. Report the following:

- The null and alternative hypotheses.
- The ANOVA table. (You may use `anova()` and omit the row for Total.)
- The value of the test statistic.
- The p-value of the test.
- A statistical decision at $\alpha = 0.05$.
- A conclusion in the context of the problem.

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

```{r, warning=FALSE}
library(faraway)
cholesterol_model <- lm(chol~weight, data=diabetes)
anova(cholesterol_model)
```

**(b)** Fit the following simple linear regression model in `R`. Use HDL as the response and weight as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `hdl_model`. Use an $F$ test to test the significance of the regression. Report the following:

- The null and alternative hypotheses.
- The ANOVA table. (You may use `anova()` and omit the row for Total.)
- The value of the test statistic.
- The p-value of the test.
- A statistical decision at $\alpha = 0.05$.
- A conclusion in the context of the problem.

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

```{r}
hdl_model <- lm(hdl~weight, data=diabetes)
anova(hdl_model)
```

## Exercise 3 (Inference "Without" `lm`)

For this exercise we will once again use the data stored in [`goalies2017.csv`](goalies2017.csv). It contains regular season (not including playoffs) career data for all 750 players in the history of the National Hockey League to play goaltender through the 2016-2017 season. The two variables we will be interested in are:

- `W` - Wins
- `MIN` - Minutes

Fit a SLR model with `W` as the response and `MIN` as the predictor. Test $H_0: \beta_1 = 0.008$ vs $H_1: \beta_1 < 0.008$ at $\alpha = 0.01$. Report the following: 

- $\hat{\beta_1}$
- $SE[\hat{\beta_1}]$
- The value of the $t$ test statistic.
- The degrees of freedom.
- The p-value of the test.
- A statistical decision at $\alpha = 0.01$.

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

You should use `lm()` to fit the model and obtain the estimate and standard error. But then you should directly calculate the remaining values. Hint: be careful with the degrees of freedom. Think about how many observations are being used.

```{r}
goalies2017 <- read.csv("goalies2017.csv")
goalies_model <- lm(W ~ MIN, data=goalies2017)
summary(goalies_model)
mean(c(1,2,3, NA), na.rm=TRUE)

beta_1 = summary(goalies_model)$coefficients[2, 1]
beta_1
se_beta_1 <- summary(goalies_model)$coefficients[2, 2]
se_beta_1
t_test_value <- (beta_1-0.008)/se_beta_1
t_test_value
degreef <- goalies_model$df.residual
degreef
p_val = pt(t_test_value, df = degreef)
p_val
```

## Exercise 4 (Simulating Sampling Distributions)

For this exercise we will simulate data from the following model:

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Where $\epsilon_i \sim N(0, \sigma^2).$ Also, the parameters are known to be:

- $\beta_0 = 4$
- $\beta_1 = 0.5$
- $\sigma^2 = 25$

We will use samples of size $n = 50$.

**(a)** Simulate this model $1500$ times. Each time use `lm()` to fit a SLR model, then store the value of $\hat{\beta}_0$ and $\hat{\beta}_1$. Set a seed using **your** UIN before performing the simulation. Note, we are simualting the $x$ values once, and then they remain fixed for the remainder of the exercise.

```{r}
uin = 671105713
set.seed(uin)
n = 50
x = seq(0, 20, length = n)
beta_0 <- rep(NA, 1500)
beta_1 <- rep(NA, 1500)
for(i in 1:1500){
  epsilon = rnorm(n, 0, 5)
  y = 4+0.5*x+epsilon
  dat = data.frame(response = y, predictor = x)
  model <- lm(response ~ predictor, data=dat)
  beta_1[i] = summary(model)$coefficients[2, 1]
  beta_0[i] = summary(model)$coefficients[1, 1]
}
```

**(b)** For the *known* values of $x$, what is the expected value of $\hat{\beta}_1$?

**(c)** For the known values of $x$, what is the standard deviation of $\hat{\beta}_1$?

```{r}
5/sum((x-mean(x))^2)^.5
```

**(d)** What is the mean of your simulated values of $\hat{\beta}_1$? Does this make sense given your answer in **(b)**?

```{r}
mean(beta_1)
```

**(e)** What is the standard deviation of your simulated values of $\hat{\beta}_1$? Does this make sense given your answer in **(c)**?

```{r}
sd(beta_1)
```

**(f)** For the known values of $x$, what is the expected value of $\hat{\beta}_0$?

**(g)** For the known values of $x$, what is the standard deviation of $\hat{\beta}_0$?

```{r}
5*(1/n+mean(x)^2/sum((x-mean(x))^2))^.5
```

**(h)** What is the mean of your simulated values of $\hat{\beta}_0$? Does this make sense given your answer in **(f)**?

```{r}
mean(beta_0)
```

**(i)** What is the standard deviation of your simulated values of $\hat{\beta}_0$? Does this make sense given your answer in **(g)**?

```{r}
sd(beta_0)
```

**(j)** Plot a histogram of your simulated values for $\hat{\beta}_1$. Add the normal curve for the true sampling distribution of $\hat{\beta}_1$.

```{r}
ggplot(data = data.frame(beta_1),
       mapping = aes(x = beta_1)) +
  geom_histogram(mapping = aes(y = ..density..),
                 color = "white",
                 bins = 25) +
  stat_function(fun = dnorm,
                args = list(mean = .5,
                            sd = 5/sum((x-mean(x))^2)^.5),
                color = "red")
```

**(k)** Plot a histogram of your simulated values for $\hat{\beta}_0$. Add the normal curve for the true sampling distribution of $\hat{\beta}_0$.

```{r}
ggplot(data = data.frame(beta_0),
       mapping = aes(x = beta_0)) +
  geom_histogram(mapping = aes(y = ..density..),
                 color = "white",
                 bins = 25) +
  stat_function(fun = dnorm,
                args = list(mean = 4,
                            sd = 5*(1/n+mean(x)^2/sum((x-mean(x))^2))^.5),
                color = "red")
```

**(l)** Create a scatterplot of the $x$ values and the fitted $y$ values from the true model. Create $1500$ new samples of size `n = 50` from the model. Each time use `lm()` to fit a SLR model, and add the fitted regression line to the plot. (The points from your original scatterplot will not longer be visible, this is okay.) Add a final line that is thicker, and a different color, for the true model.

```{r}
uin = 671105713
set.seed(uin)
n = 50
x = seq(0, 20, length = n)
gg <- ggplot()
for(i in 1:100){
  epsilon = rnorm(n, 0, 5)
  y = 4+0.5*x+epsilon
  dat = data.frame(response = y, predictor = x)
  model <- lm(response ~ predictor, data=dat)
  y_pred = predict(model)
  dat1 = data.frame(y_pred = y_pred, predictor = x)
  gg = gg +
    geom_point(data = dat1,
               mapping = aes(x = predictor,
                             y = y_pred))
}
gg + geom_abline(slope = .5, intercept = 4, color = "red", size = 1)
```

## Exercise 5 (Simulating Confidence Intervals)

For this exercise we will simulate data from the following model:

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Where $\epsilon_i \sim N(0, \sigma^2).$ Also, the parameters are known to be:

- $\beta_0 = 1$
- $\beta_1 = 3$
- $\sigma^2 = 16$

We will use samples of size $n = 20$.

Our goal here is to use simulation to verify that the confidence intervals really do have their stated confidence level.

**(a)** Simulate this model $2000$ times. Each time use `lm()` to fit a SLR model, then store the value of $\hat{\beta}_0$ and $s_e$. Set a seed using **your** UIN before performing the simulation. Note, we are simualting the $x$ values once, and then they remain fixed for the remainder of the exercise.

```{r}
uin = 671105713
set.seed(uin)
n = 20
x = seq(-5, 5, length = n)
lower_90 <- rep(NA, 2000)
upper_90 <- rep(NA, 2000)
beta_0 <- rep(NA, 2000)
sigma_beta_0 <- rep(NA, 2000)
for(i in 1:2000){
  epsilon = rnorm(n, 0, 4)
  y = 1+3*x+epsilon
  dat = data.frame(response = y, predictor = x)
  model <- lm(response ~ predictor, data=dat)
  lower_90[i] = confint(model, parm = "(Intercept)", level = .9)[1]
  upper_90[i] = confint(model, parm = "(Intercept)", level = .9)[2]
  sigma = summary(model)$sigma
  sigma_beta_0[i] = sigma*(1/n+mean(x)^2/sum((x-mean(x))^2))^2
  beta_0[i] = summary(model)$coefficients[1,1]
}

t = qt(1-.1/2, 18)
lower = beta_0-t*sigma_beta_0
upper = beta_0+t*sigma_beta_0
```

**(b)** For each of the $\hat{\beta}_0$ that you simulated calculate a 90% confidence interval. Store the lower limits in a vector `lower_90` and the upper limits in a vector `upper_90`. Some hints:

- You will need to use `qt()` to calculate the critical value, which will be the same for each interval.
- Remember that `x` is fixed, so $S_{xx}$ will be the same for each interval.
- You could, but do not need to write a `for` loop. Remember vectorized operations.

```{r}

```

**(c)** What proportion of these intervals contain the true value of $\beta_0$?

```{r}
mean(lower_90 <= 1 & upper_90 >=1)
```

**(d)** Based on these intervals, what proportion of the simulations would reject the test $H_0: \beta_0 = 0$ vs $H_1: \beta_0 \neq 0$ at $\alpha = 0.10$?

```{r}
1 - mean(lower_90 <= 0 & upper_90 >= 0)
```

**(e)** For each of the $\hat{\beta}_0$ that you simulated calculate a 99% confidence interval. Store the lower limits in a vector `lower_99` and the upper limits in a vector `upper_99`.

```{r}
set.seed(uin)
lower_99 <- rep(NA, 2000)
upper_99 <- rep(NA, 2000)
for(i in 1:2000){
  epsilon = rnorm(n, 0, 4)
  y = 1+3*x+epsilon
  dat = data.frame(response = y, predictor = x)
  model <- lm(response ~ predictor, data=dat)
  lower_99[i] = confint(model, parm = "(Intercept)", level = .99)[1]
  upper_99[i] = confint(model, parm = "(Intercept)", level = .99)[2]
}
```

**(f)** What proportion of these intervals contain the true value of $\beta_0$?

```{r}
mean(lower_99<=1 & upper_99>=1)
```

**(g)** Based on these intervals, what proportion of the simulations would reject the test $H_0: \beta_0 = 0$ vs $H_1: \beta_0 \neq 0$ at $\alpha = 0.01$?

```{r}
1 - mean(lower_99 <= 0 & upper_99 >= 0)
```
